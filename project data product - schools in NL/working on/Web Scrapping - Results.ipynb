{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWD83a3ZYw2b"
   },
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Web Scraping Tools](#toc1_)    \n",
    "- [Selenium](#toc2_)    \n",
    "  - [Case study: Scraping Linkedin job posts](#toc2_1_)    \n",
    "    - [Install web driver](#toc2_1_1_)    \n",
    "    - [Log into Linkedin](#toc2_1_2_)    \n",
    "    - [Find job position](#toc2_1_3_)    \n",
    "      - [What is the job position you want to search for?](#toc2_1_3_1_)    \n",
    "      - [What is the job location you want to search for?](#toc2_1_3_2_)    \n",
    "      - [Can we find what we need from the HTML?](#toc2_1_3_3_)    \n",
    "  - [Extra: Do the scraping using Selenium](#toc2_2_)    \n",
    "  - [Extra: Save cookies in a pickle ðŸ¥’](#toc2_3_)    \n",
    "- [References & Acknowledgments](#toc3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn4_Ir5FYw2f"
   },
   "source": [
    "# <a id='toc1_'></a>[Web Scraping Tools](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w94RNGj7Yw2f"
   },
   "source": [
    "Some of the main tools used for web scraping in Python include:\n",
    "- [`requests`](https://requests.readthedocs.io/en/latest/) - allows you to send HTTP requests easily through built-in structures that mimic the typical HTTP request structure, e.g. `get`, `post`, etc. It's basically the starting point for any web scraping project. However, it has 2 drawbacks: it can only scrape **static** HTML content and it sends **synchronous** requests. This means that it doesn't work well on Javascript heavy pages (i.e. pages with a lot of dynamic content, like `AirBnB`) and it becomes very slow if you want to send a big number of requests.\n",
    "- `BeautifulSoup` - allows you to extract information from HTML pages using the HTML/CSS structural elements, i.e. tags and attributes.\n",
    "- `Scrapy` - automates web scraping by providing some of the typical structures for extracting information from websites. It is **asynchronous** and widely used for large scale scraping projects. Drawbacks: It runs on **static** HTML pages and it requires a decent understanding of object-oriented programming.\n",
    "- `Selenium` - emulates web browsers to enable scraping of Javascript-heavy websites. Drawbacks: It can be slow on its own so it's typically used with `requests`, `BeautifulSoup`, and/or `Scrapy`.\n",
    "- `aiohttp` - the **asynchronous** cousin of `requests`. Has mostly the same functionality but it doesn't wait for each request to receive a response from the server before sending the next request - i.e. why it's asynchronous. To understand how asynchronous programming works, I highly recommend this [blog post series on the `asyncio` library](https://bbc.github.io/cloudfit-public-docs/asyncio/asyncio-part-1). Please read this **after** the bootcamp though, you likely won't need it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RziWj2zGYw2g"
   },
   "source": [
    "**Note:** Remember, before wanting to scrape any website (and especially big websites), make sure that there's an API available that you can use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgU-47ssYw2g"
   },
   "source": [
    "# <a id='toc2_'></a>[Selenium](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHJmjRFpYw2g"
   },
   "source": [
    "> Selenium is an open-source framework **widely used for testing web applications**. It empowers developers and testers to automate interactions with web applications, such as clicking buttons, filling forms, and navigating pages, mimicking user behavior. It supports interaction with complex web elements and dynamic content, making it suitable for modern web applications.\n",
    "\n",
    "(courtesy of ChatGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEmRMtG5Yw2g"
   },
   "source": [
    "## <a id='toc2_1_'></a>[Case study: Scraping Linkedin job posts](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZCRKx-gYw2h"
   },
   "source": [
    "![Job](https://media.giphy.com/media/dgg13lkNAUa5eibLiY/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Cg6Ybi4-JXLo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (4.13.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from selenium) (0.22.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.3)\n",
      "Requirement already satisfied: idna in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Collecting webdriver-manager\n",
      "  Using cached webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from webdriver-manager) (22.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ana galvao\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2022.12.7)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.0 webdriver-manager-4.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "K5fblUwuR9V-",
    "outputId": "b877873d-81d3-4ddc-d881-c2e76167c84a"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from getpass import getpass\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common import NoSuchElementException, ElementClickInterceptedException\n",
    "import pathlib\n",
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "# Ignore warning -- Some methods are going to be deprecated\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSHkZub5Yw2i"
   },
   "source": [
    "### <a id='toc2_1_1_'></a>[Install web driver](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "I5HHJRnxR9WA",
    "outputId": "e8617a80-9fb2-41b5-f3b7-246d5eeff06b"
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATnYxlDUYw2j"
   },
   "source": [
    "### <a id='toc2_1_2_'></a>[Log into Linkedin](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "hg4HLHuER9WB"
   },
   "outputs": [],
   "source": [
    "# open the website\n",
    "driver.get('https://scholenopdekaart.nl/basisscholen/almere/21141/ikc-sterrenschool-de-ruimte/resultaten/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"ac263afa8c763e51bb1886b59c63ca1f\", element=\"10A942E7129E1C66FC86B236252AC179_element_11\")>\n"
     ]
    }
   ],
   "source": [
    "button_search = driver.find_element(By.CLASS_NAME, 'CybotCookiebotDialogBodyButton')\n",
    "button_search.click()\n",
    "time.sleep(random.random() * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cookies in a pickle file\n",
    "import pickle\n",
    "\n",
    "# Create an empty folder\n",
    "cookies_dir = 'saved_cookies'\n",
    "lis_dir = os.listdir()\n",
    "\n",
    "if cookies_dir not in lis_dir:\n",
    "    os.mkdir(cookies_dir)\n",
    "else:\n",
    "    pass # os.removedirs(cookies_dir) --> to remove a directory\n",
    "\n",
    "save_location = cookies_dir + '/cookies.pkl'\n",
    "pickle.dump(driver.get_cookies() , open(save_location,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cookies\n",
    "cookies = pickle.load(open(save_location, \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'95%'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_fundlevel = driver.find_element(By.CLASS_NAME, 'highcharts-label').get_attribute(\"textContent\")\n",
    "result_fundlevel\n",
    "#result_fundlevel_simschools = driver.find_element(By.CLASS_NAME, 'highcharts-label').get_attribute(\"textContent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z3jz26TYw2j"
   },
   "outputs": [],
   "source": [
    "# Add email\n",
    "email = input('Enter your email: ')\n",
    "\n",
    "# Find email box\n",
    "email_box = driver.find_element(By.ID, \"username\")\n",
    "\n",
    "# Clear email box\n",
    "email_box.clear()\n",
    "\n",
    "# Input password into browser\n",
    "email_box.send_keys(email)\n",
    "\n",
    "# Add sleeping time to mimic human behaviour\n",
    "time.sleep(random.random() * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qZnNO0wYw2j"
   },
   "outputs": [],
   "source": [
    "# Add password\n",
    "password = getpass('Enter your password: ')\n",
    "\n",
    "# Find password box\n",
    "pass_box = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "# Clear password box\n",
    "pass_box.clear()\n",
    "\n",
    "# Input password into browser\n",
    "pass_box.send_keys(password)\n",
    "\n",
    "# Add sleeping time to mimic human behaviour\n",
    "time.sleep(random.random() * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKpjIOqFYw2k"
   },
   "outputs": [],
   "source": [
    "# Find and click on the log-in button\n",
    "login = driver.find_element(By.CLASS_NAME, 'login__form_action_container')\n",
    "login.click()\n",
    "time.sleep(random.random() * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghIm1vlzJXLz"
   },
   "outputs": [],
   "source": [
    "# Add exception handling\n",
    "try:\n",
    "    login = driver.find_element(By.CLASS_NAME, 'login__form_action_container')\n",
    "    login.click()\n",
    "    time.sleep(random.random() * 3)\n",
    "except NoSuchElementException:\n",
    "    print(\"Log-in already done!\")\n",
    "except Exception as e:\n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0UawZiSYw2k"
   },
   "source": [
    "### <a id='toc2_1_3_'></a>[Find job position](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bQEVcYPmR9WE"
   },
   "outputs": [],
   "source": [
    "# Go to job search bar\n",
    "try:\n",
    "    job_icon = driver.find_element(By.CSS_SELECTOR, \"span[title='Jobs']\")\n",
    "    job_icon.click()\n",
    "    time.sleep(random.random() * 3)\n",
    "except ElementClickInterceptedException:\n",
    "    print(\"Element not displayed by JS. Try zooming in or resizing the window\")\n",
    "except Exception as e:\n",
    "    print(repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdY-6iaPYw2l"
   },
   "outputs": [],
   "source": [
    "# Zooming in\n",
    "driver.execute_script(\"document.body.style.zoom='200%'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A5GKTKZYw2l"
   },
   "outputs": [],
   "source": [
    "# Zooming out\n",
    "driver.execute_script(\"document.body.style.zoom='67%'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc9ncMZkYw2l"
   },
   "source": [
    "#### <a id='toc2_1_3_1_'></a>[What is the job position you want to search for?](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tsjAlTbYw2l"
   },
   "outputs": [],
   "source": [
    "# Optional - Change window size\n",
    "# driver.set_window_size(800, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aksOK7YbR9WF"
   },
   "outputs": [],
   "source": [
    "search_job = driver.find_elements(By.CLASS_NAME,'jobs-search-box__text-input')[0]\n",
    "job = input('What job do you want to search for: ')\n",
    "search_job.clear()\n",
    "search_job.send_keys(job)\n",
    "time.sleep(random.random() * 3)\n",
    "\n",
    "# Go to the location tab\n",
    "search_job.send_keys(Keys.TAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz7Fv_LcYw2l"
   },
   "source": [
    "#### <a id='toc2_1_3_2_'></a>[What is the job location you want to search for?](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqHdk1eJR9WG"
   },
   "outputs": [],
   "source": [
    "location_box = driver.switch_to.active_element\n",
    "location = input('Where do you want to search for jobs: ')\n",
    "location_box.send_keys(location)\n",
    "time.sleep(random.random() * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oehwUBoFR9WH"
   },
   "outputs": [],
   "source": [
    "# Now let's search\n",
    "location_box.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viIYrLkJYw2m"
   },
   "source": [
    "**Note:** In this exercise I'm keeping both windows open at the same time alongside each other. If you switch from one window to another the same strategy won't work.  \n",
    "\n",
    "**Why?** Because Linkedin will close the location search bar as soon as you switch to VSCode.\n",
    "\n",
    "**Why do they do that?** Probably to get rid of us :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCvypbbnR9WI"
   },
   "outputs": [],
   "source": [
    "# Maximize the window - useful to see all the elements as the page is dynamic\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLcFYxe5Yw2m"
   },
   "outputs": [],
   "source": [
    "## Optional: you can also fullscreen the window\n",
    "# driver.fullscreen_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFaJI8TOYw2m"
   },
   "source": [
    "#### <a id='toc2_1_3_3_'></a>[Can we find what we need from the HTML?](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR1rjMlpYw2n"
   },
   "source": [
    "As mentioned previously, Selenium can be quite slow, so we'd always want to check whether we can fetch our data directly using static web scraping tools (i.e. `requests`, `BeautifulSoup`, `scrapy`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INf0pzQGYw2n"
   },
   "outputs": [],
   "source": [
    "# Check if the source code contains the job listings\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html)\n",
    "soup.find_all(attrs={'class': re.compile(r'job-card-list__title')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_3q4qSpYw2n"
   },
   "outputs": [],
   "source": [
    "# Clean the list\n",
    "job_list_dirty = soup.find_all(attrs={'class': re.compile(r'job-card-list__title')})\n",
    "job_list_clean = [job.text.strip() for job in job_list_dirty]\n",
    "job_list_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXkkkJavYw2o"
   },
   "outputs": [],
   "source": [
    "# Do the same for the company\n",
    "job_company_dirty = soup.find_all('div', attrs={'class': re.compile(r'^artdeco-entity-lockup__subtitle')})\n",
    "job_company_clean = [company.text.strip() for company in job_company_dirty]\n",
    "job_company_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAo_dkNwYw2o"
   },
   "outputs": [],
   "source": [
    "# Make it into a dataset\n",
    "data = zip(job_list_clean, job_company_clean)\n",
    "df = pd.DataFrame(data, columns=['Job', 'Company'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWHncrUpYw2o"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Great, let's now create a function out of this:\n",
    "def get_job_postings(driver, page):\n",
    "\n",
    "     # Zoom in 100% to ensure all HTML is loaded\n",
    "     driver.execute_script(\"document.body.style.zoom='100%'\")\n",
    "\n",
    "     # Go to bottom of page to retrieve all job postings\n",
    "     page.send_keys(Keys.END)\n",
    "     page.send_keys(Keys.CONTROL + Keys.HOME) # combination of the two keys brings you to the top of the element\n",
    "\n",
    "     # Parse HTML\n",
    "     html = driver.page_source\n",
    "     soup = BeautifulSoup(html)\n",
    "\n",
    "     # Get jobs\n",
    "     job_list_dirty = soup.find_all(attrs={'class': re.compile(r'job-card-list__title')})\n",
    "     job_list_clean = [job.text.strip() for job in job_list_dirty]\n",
    "\n",
    "     # Get companies\n",
    "     job_company_dirty = soup.find_all('div', attrs={'class': re.compile(r'^artdeco-entity-lockup__subtitle')})\n",
    "     job_company_clean = [company.text.strip() for company in job_company_dirty]\n",
    "\n",
    "     # Convert data in to dataframe\n",
    "     data = zip(job_list_clean, job_company_clean)\n",
    "     return pd.DataFrame(data, columns=['Job', 'Company'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FxEM_9CYw2p"
   },
   "outputs": [],
   "source": [
    "page = driver.find_element(By.CSS_SELECTOR,\"a[class^='disabled ember-view']\")\n",
    "get_job_postings(driver, page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_F0HnttjYw2p"
   },
   "source": [
    "#### Loop through the available pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZNZ485GYw2q"
   },
   "outputs": [],
   "source": [
    "# Get a list with the buttons in the page\n",
    "def get_buttons(page):\n",
    "    buttons = []\n",
    "    for button in page.find_elements(By.XPATH, \"//ul/li/button\"):\n",
    "        try:\n",
    "            int(button.text)\n",
    "            buttons.append(button)\n",
    "        except:\n",
    "            pass\n",
    "    return buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xUb-vkJYw2q"
   },
   "outputs": [],
   "source": [
    "# Get the number of pages to scrape\n",
    "current_page = driver.find_element(By.CSS_SELECTOR,\"a[class^='disabled ember-view']\")\n",
    "buttons = get_buttons(current_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ckxwa06Yw2r"
   },
   "outputs": [],
   "source": [
    "# Loop through pages and save results in a dataframe\n",
    "df = pd.DataFrame()\n",
    "driver.execute_script(\"document.body.style.zoom='100%'\")\n",
    "\n",
    "for i in range(len(buttons)):\n",
    "    # Printing the button number for debugging purposes\n",
    "    print(i)\n",
    "\n",
    "    # Extract posts from current page\n",
    "    current_page = driver.find_element(By.CSS_SELECTOR,\"a[class^='disabled ember-view']\")\n",
    "    postings = get_job_postings(driver, current_page)\n",
    "\n",
    "    # Refresh button list (if you don't the code will throw an exception.. trust me I spent half an hour debugging it)\n",
    "    current_buttons = get_buttons(current_page)\n",
    "\n",
    "    # Add to dataframe\n",
    "    df = pd.concat([df, postings], axis=0)\n",
    "\n",
    "    # Go to the next page\n",
    "    current_buttons[i].click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "czjvtbmgYw2r"
   },
   "outputs": [],
   "source": [
    "# Check dataframe\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXjpkB6gYw2s"
   },
   "source": [
    "## <a id='toc2_2_'></a>[Extra: Do the scraping using Selenium](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBWMiRwmYw2s"
   },
   "source": [
    "This bit is to illustrate how slow Selenium can be in comparison to retrieving the HTML for the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vWSEDOxdYw2s"
   },
   "outputs": [],
   "source": [
    "def page_scraper(job_no): ## add pages\n",
    "\n",
    "    \"\"\" SUMMARY: This function retrieves all the job posts links from one page and returns a dataset with\n",
    "    the name of the job in one column and the link to the post in the other. Also it will write the same info in different files for every single job post.\n",
    "\n",
    "    HOW IT WORKS: Input the number of jobs you want to scrape. It will search on the page for the elements by css selector\n",
    "    from all the job posts then loop for every single element and retrieve the 'href'. Also it will click on every job post and find the job name.\n",
    "    This info will be saved in a dictionary that will in the end be converted to a dataset.\n",
    "    Below we will open and create a text file with the name of the job post and inside save the link for further details\"\"\"\n",
    "\n",
    "    # For scraper reasons it's required to duplicate the job_no as it retrieves 2 times the same position:\n",
    "    #job_no = job_no*2\n",
    "\n",
    "    # empty list for saving the job names , link and extra info:\n",
    "    job_list = []\n",
    "\n",
    "    # Reduce the page size in order to be able to find the name of the job in the right session\n",
    "    driver.execute_script(\"document.body.style.zoom='67%'\")\n",
    "\n",
    "    # all jobs in the page\n",
    "    job_raw = driver.find_elements(By.CSS_SELECTOR,\"a[class^='disabled ember-view']\")\n",
    "\n",
    "    # go to the end of the page for all the elements to be loaded\n",
    "    page = driver.find_element(By.CSS_SELECTOR,\"a[class^='disabled ember-view']\")\n",
    "    page.send_keys(Keys.END)\n",
    "    # go to the top of the page for all the elements to be loaded\n",
    "    page.send_keys(Keys.CONTROL + Keys.HOME) # combination of the two keys brings you to the top of the element\n",
    "\n",
    "\n",
    "    for job_index in range(job_no):\n",
    "        # get the job link\n",
    "        ref = job_raw[job_index].get_attribute('href')\n",
    "        time.sleep(random.random() * 3)\n",
    "\n",
    "        # increase the page size because the inspect for getting the job name where done wiht the page maximized\n",
    "        driver.execute_script(\"document.body.style.zoom='100%'\")\n",
    "\n",
    "        ## let's click on the job post ##\n",
    "        # driver.find_elements_by_css_selector(\"a[class^='disabled ember-view']\")[job_index].click()\n",
    "        job_raw[job_index].click()\n",
    "        time.sleep(random.random() * 3)\n",
    "\n",
    "        ## then we reduce the page size in order to be able to see the right part of the page\n",
    "        # and find the element with the name of the job ##\n",
    "        driver.execute_script(\"document.body.style.zoom='67%'\")\n",
    "        time.sleep(random.random() * 3)\n",
    "\n",
    "        # get the job name with the .text method\n",
    "        job_name = driver.find_element(By.CSS_SELECTOR, \"h2[class^='t-24 t-bold']\").text\n",
    "        time.sleep(random.random() * 3)\n",
    "\n",
    "        # Couldn't retrieve the company name with the same method so created a workaround\n",
    "        company_name = \" \".join(driver.find_element(By.CSS_SELECTOR, \"a[href^='/company']\").get_attribute('href').split(\"/\")[-3].split(\"-\")).title()\n",
    "        print(company_name)\n",
    "\n",
    "        # get company name:\n",
    "        job_details = driver.find_element(By.ID, \"job-details\").text\n",
    "\n",
    "        # increase the page size:\n",
    "        #driver.execute_script(\"document.body.style.zoom='100%'\")\n",
    "\n",
    "        # populate list:\n",
    "        job_idx_list = [ref, job_name, company_name, job_details]\n",
    "        time.sleep(random.random() * 3)\n",
    "\n",
    "        page.send_keys(Keys.PAGE_DOWN)\n",
    "        job_list.append(job_idx_list)\n",
    "        print(f\"Collected job: {job_name} for company: {company_name}\")\n",
    "\n",
    "    #Create dataframe:\n",
    "    job_df = pd.DataFrame(job_list,\n",
    "                                 columns = [\"job_link\", \"position\", \"company name\", \"job description\"]\n",
    "                                ).drop_duplicates()\n",
    "\n",
    "\n",
    "    #Save dataframe in excel file to later use our job\n",
    "    job_df.to_excel(pathlib.Path().joinpath('scraped_jobs.xlsx'),\n",
    "                           sheet_name='Jobs',\n",
    "                           index= False)\n",
    "\n",
    "    return job_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "DMD6fmemYw2t",
    "outputId": "7654e282-8e1b-4645-e47e-7489c0d8cba2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-97b6e1c0d27b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Here I input the number of jobs to reduce the collection time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpage_scraper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-1feb66363cfe>\u001b[0m in \u001b[0;36mpage_scraper\u001b[0;34m(job_no)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Reduce the page size in order to be able to find the name of the job in the right session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"document.body.style.zoom='67%'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# all jobs in the page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "# Here I input the number of jobs to reduce the collection time\n",
    "page_scraper(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ju3aWJwMYw2t"
   },
   "source": [
    "If you work in Jupyter notebook, you can use the magic function `%time` before your function to check how long it took to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "h6oC7Z9RYw2t",
    "outputId": "06b224f3-e583-4add-9dc5-af9171ced444"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1feb66363cfe>\u001b[0m in \u001b[0;36mpage_scraper\u001b[0;34m(job_no)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Reduce the page size in order to be able to find the name of the job in the right session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"document.body.style.zoom='67%'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# all jobs in the page\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "%time page_scraper(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 174
    },
    "id": "vKPma0f8Yw2t",
    "outputId": "3ddcd5af-284b-4b8e-9848-2c2e72cdaabb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-47e71f503966>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# closes the driver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "driver.close() # closes the driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYw5cWn8Yw2u"
   },
   "source": [
    "## <a id='toc2_3_'></a>[Extra: Save cookies in a pickle ðŸ¥’](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMs76OeBR9WM"
   },
   "outputs": [],
   "source": [
    "# Save cookies in a pickle file\n",
    "import pickle\n",
    "\n",
    "# Create an empty folder\n",
    "cookies_dir = 'saved_cookies'\n",
    "lis_dir = os.listdir()\n",
    "\n",
    "if cookies_dir not in lis_dir:\n",
    "    os.mkdir(cookies_dir)\n",
    "else:\n",
    "    pass # os.removedirs(cookies_dir) --> to remove a directory\n",
    "\n",
    "save_location = cookies_dir + '/cookies.pkl'\n",
    "pickle.dump(driver.get_cookies() , open(save_location,\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9bSN73CR9WN"
   },
   "outputs": [],
   "source": [
    "# Load cookies\n",
    "cookies = pickle.load(open(save_location, \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o7cL9x6Yw2u"
   },
   "source": [
    "# <a id='toc3_'></a>[References & Acknowledgments](#toc0_)\n",
    "\n",
    "Thanks Goncalo Jardim for the main class structure and code to scrape Linkedin job posts."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "DjHVfaipR9WG",
    "gM6LkxT5R9WH"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
